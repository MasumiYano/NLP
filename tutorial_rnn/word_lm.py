# -*- coding: utf-8 -*-
"""Word_LM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PTaT8MOvsPqGke8FzqtyyXaipD_zDSkL
"""
'''
Author: Raghavi Sakpal
Date: June, 2024
Description: Word-based text generation (Language Model Problem) using RNN and LSTM.
             Input = sequence length, Output = one word out
             Dataset = rhyme.txt
             This code is using the same technique as Character-based LM.
Note: We need punctuations for text generation and no need to make words case-sensitive.
'''

import numpy as np
import tensorflow as tf


# Code to load the document in memory
def load_doc(filename):
    file = open(filename, 'r')
    text = file.read()
    file.close()
    return text


# Load the rhyme.txt file and print it to the screen
raw_text = load_doc('rhyme.txt')
print(raw_text)

# Step 1: Create vocabulary of words
words = raw_text.split()
vocab = sorted(list(set(words)))
print(words)
print(len(words))
print(vocab)
print(len(vocab))

# Encode words to integers using same technique as Character-based LM
word_to_index = {word: i for i, word in enumerate(vocab)}
print(word_to_index)
index_to_word = {i: word for i, word in enumerate(vocab)}
print(index_to_word)

# Step 2: Create input sequences and corresponding labels
seq_length = 2
sequences = []
labels = []

for i in range(len(words) - seq_length):
    # Select the sequences of words
    seq = words[i:i + seq_length]
    print(seq)
    # Select the next output word
    label = words[i + seq_length]
    # Store the sequences & labels
    sequences.append([word_to_index[word] for word in seq])
    labels.append(word_to_index[label])

print(sequences)
print(len(sequences))
print(labels)

# Step 3: Encode text to one-hot vector representation
# Convert the sequences & labels into numpy arrays
X = np.array(sequences)
Y = np.array(labels)

# Convert the sequences & labels into one-hot vector representation
X = tf.keras.utils.to_categorical(X, num_classes=len(vocab))
Y = tf.keras.utils.to_categorical(Y, num_classes=len(vocab))

print(X.shape)
print(X)

from os import access


# Step 4.a: Define a RNN model for Word-based text generation
# Model 1: Simple RNN with Relu and softmax functions
def model_1(X):
    model = tf.keras.Sequential()
    model.add(tf.keras.layers.SimpleRNN(75, input_shape=(X.shape[1], X.shape[2]), activation='relu'))
    model.add(tf.keras.layers.Dense(len(vocab), activation='softmax'))

    '''Compile the model for training using the following parameters:
  Optimization: Adam optimizer (used to minimize the loss function during the training process)
  Loss Function: Categorical Crossentropy
  Metrics: Accuracy'''
    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
    # summarize defined model
    model.summary()
    tf.keras.utils.plot_model(model, to_file='model_RNN1.png', show_shapes=True)
    return model


# Model 2: LSTM and softmax function
def model_2(X):
    model = tf.keras.Sequential()
    model.add(tf.keras.layers.LSTM(75, input_shape=(X.shape[1], X.shape[2])))
    model.add(tf.keras.layers.Dense(len(vocab), activation='softmax'))

    '''Compile the model for training using the following parameters:
  Optimization: Adam optimizer (used to minimize the loss function during the training process)
  Loss Function: Categorical Crossentropy
  Metrics: Accuracy'''
    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
    # summarize defined model
    model.summary()
    tf.keras.utils.plot_model(model, to_file='model_LSTM1.png', show_shapes=True)
    return model


# Step 5: Train the model using input sequences (X) and corresponding labels (Y) for 100 epochs (cycles)
model_RNN = model_1(X)
model_LSTM = model_2(X)

model_RNN.fit(X, Y, epochs=100, verbose=0)
model_LSTM.fit(X, Y, epochs=100, verbose=0)

# Step 6: Model Prediction
start_seq = 'king was in'


# Function to generate the next set of words based on the initial seed text
def generate_text(model, vocab_length, seq_length, seed_text, n_words):
    output_text = seed_text
    output = output_text.split()
    for i in range(n_words):
        x = np.array([[word_to_index[word] for word in output[-seq_length:]]])
        x_one_hot = tf.keras.utils.to_categorical(x, num_classes=vocab_length)
        # Use the trained model to make prediction
        prediction = model.predict(x_one_hot, verbose=0)
        next_index = np.argmax(prediction)
        next_word = index_to_word[next_index]
        output_text += ' ' + next_word

    return output_text


# Generate text using both RNN & LSTM
generated_text_RNN = generate_text(model_RNN, len(vocab), seq_length, start_seq, 5)
print("Generate Text using RNN: ", generated_text_RNN)
# Generate text using both RNN & LSTM
generated_text_LSTM = generate_text(model_LSTM, len(vocab), seq_length, start_seq, 5)
print("Generate Text using LSTM: ", generated_text_LSTM)
